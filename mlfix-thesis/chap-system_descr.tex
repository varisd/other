\chapter{System description}

In this chapter we are going to describe the main components
of the MLFix system and take a closer look at the suggested processing pipeline.

The main idea of the MLFix system is to analyze the provided input data providing
a set of input features 

\section{Processing pipeline}

The MLFix is, similarly to its predecessor, almost entirely implemented in the
Treex\cite{Popel:2010:TMN:1884371.1884406}\footurl{http://ufal.mff.cuni.cz/treex}
framework (formerly knownas TectoMT). The framework was
originally created as deep dependency-based English-Czech
translation system however, due to its modularity, it is
now used for various NLP processing tasks across different
languages. The framework is closely related to the theory of Functional Generative Description\cite{Sgall1967}
and was adapted to the Prague Dependency Treebank\cite{pdt20:2006}.
It defines three layers of language representation: the morphological layer, the analytical layer
and the tectogrammatical layer.
Since the tools available for tectogrammatical layer analysis are usually
language specific and implemented only for a limited set of languages
we decided to use only the other two for the representation
of the analysed data.

\subsection{M-Layer analysis}

% TODO: pospat nastroje
The MLFix analysis pipeline is derived from an existing Depfix pipeline
with a several modifications to make it easier to apply to different
target languages. The input data (source sentence + MT output aligned on the sentence level)
is first read in parallel and stored into the Treex internal representation.
Both source side and MT side are tokenized by a rule-based tokenizer. After
that, lemmatization and part-of-speech (POS) tagging is performed on each sentence. The
assigned POS tags are then transformed to the unified format. Optionally,
a named-entity recognition can be applied but it isn't mandatory.
%footnote: we need more than coarse tags

\subsection{Interset}

Since there are usually different tagsets used for each individual language
we would be forced to modify our existing pipeline with each new language.
Therefore we decided to use Interset\cite{biblio:ZeReusableTagset2008}\footurl{https://ufal.mff.cuni.cz/interset}, an interlingua-based
representation of POS tags from various tagsets. This way, in the following steps of the analysis,
the choice of a specific tagset becomes transparent because our blocks only
have to deal with one well-defined set of features.

\subsection{Word alignment}

For the next step, we create a word-level alignment between each sentence pair
using GIZA++\cite{och:ney:2000}. We make one-to-one word alignment where possible
using intersection symmetrization. This steps helps us later with feature extraction
and with further processing of the target sentence.

\subsection{A-Layer analysis}

% TODO: popsat nastroje
Since we do not often have a reliable dependency parser available for many languages,
we decided to perform dependency parsing only on the source (English) side.
Still, research on Depfix have shown that the knowledge of the dependency structure
of the target side sentences can significantly improve post-editor performance.
For example, the dependency relations between the sentence nodes can help us identify
incorrect agreement. For this reason, we decided to create the target side dependency
structures simply by projecting the source sentence structure onto the target side
using the word alignment.
Such dependency structure will most likely be at least partially incorrect but we
think that this additional information by being consistent for a given language pair
can be of help for our statistical component.

\subsection{Training pipeline}

We also do preprocessing of data we use for training our statistical component.
The pipeline is similar to the processing pipeline with addition of processing
the reference sentences. The reference sentences are processed in a same way
as the MT output, however they are not analyzed on the a-layer, only a simple
lemma based word alignment with the MT output is made.

\section{Statistical component}

After the data preprocessing we can extract all the available features and
call the trained model. The features are extracted separetely for each node and passed
to the model. The model should be able to accomplish these two goals:
\begin{enumerate}
    \item identify the candidates with incorrect morphology,
    \item fix the incorrect morphological features.
\end{enumerate}
Of course, these two steps can be split between multiple separate components (and models),
we describe our post-editing process in more detail later in section XXX.

We decided to use Scikit-Learn toolkit to train and executed our models since
it has easy-to-use and quite uniform interface. However, our component can be easily
extended to support other implementations of machine learning methods (e.g. VowpalWabbit).
%TODO: footnote - there are wrappers for VW in treex already
%TODO: doplint odkaz

\section{Wordform generation}

% sloucit s predchozi sekci?
When we are done with identifying the morphologically incorrect words and assigning
them a new morphological categories we need to regenerate their surface form.
This can be done either by rule-based component (for Czech language) or with
help of another statistical model
%footnote: flect?

\section{Language independence}

From the previous description, we can notice that MLFix still depends on several
language specific tools. It is quite dependent on the capability of the source
language analysis, because we do not only need a POS tagger but also a dependency
parser. We however assume that the source sentences are usually grammatical correct
so it is much easier to provide the required tools than their modified versions
targeted at the MT output.

We also require a specific decoder of the source and target POS tags into the Interset
structure but Interset already covers many of the most widespread tagsets available
and its support is still growing.
%TODO: zminit universal dependencies?

Finaly, aside from a language specific post-editing models (which we do not expect
to be reusable across different languages) we require a module that regenerates
the correct word form (either from the lemma+tag or lemma+Interset features combination).
Such tool might not be explicitly available for every language but if no
other option is provided we suggest training a statistical form generator.
In our case, we used Flect\footurl{https://github.com/UFAL-DSG/flect}, a Scikit-Learn-based
tool that learns morphological inflection patterns from annotated corpora.
%TODO: blize popsat?
