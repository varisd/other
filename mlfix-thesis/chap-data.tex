\chapter{Available Data}
\label{chap:data}

% dostupna data
% mnozstvi
% popis
% vyuziti

%XXX TOTO ZREJME AZ DO POPISU SYSTEMU
%In this chapter we describe the format of the input data, that
%our system require for training and present all the available datasets.
%The differences between the suggested data can seem minor but they can
%have impact on the overall performance of the system.

%To train our system, we require sentence-level aligned set of source sentences,
%MT output and a reference sentences
%TODO: footnote - reference in this thesis as a triparallel data
%. The reference sentences can be either
%a classic reference translation of the source text or a result of a human
%post-editation of the MT output, they are however required because they
%provide use with the possible corrections of the MT output.
%Naturally, the closer the MT output is
%to the reference translation, the easier it should be for our system
%to extract valuable learning instances form the sentences. We also
%considered using only pair of sentence aligned pair of MT translated
%sentences and the reference sentences however, this way the trained
%model loses some of the useful features which can be extracted from
%the triparallel data.
%XXX

In this chapter we take a closer look at the available
sources of data and describe how they contributed to our research.

We came across a various sources of training data with various level
of usefulness. The data was usually available only in a smaller volume. Some of the sources
are:
\begin{itemize}
\item Khan academy\footurl{https://khanovaskola.cz/} human
post-edits of manually translated (EN$\rightarrow$CS) subtitles,
\item Autodesk
triparallel data\footurl{https://autodesk.app.box.com/Autodesk-PostEditing},
\item log files of human post-editing done by Lingea for the
Health in my Language\footurl{http://www.himl.eu/} (HimL) project test dataset,

% uvest QT21?
%\item data from the QT21 project\footurl{http://www.qt21.eu/deliverables/annotations/},

%TODO: wmt - odkaz, footnote?
\item results from the previous workshops on machine translation (mainly\linebreak
WMT10 \citep{callisonburch-EtAl:2010:WMT}\footurl{http://www.statmt.org/wmt10/},
and WTM16 \citep{bojar-EtAl:2016:WMT1}\footurl{http://www.statmt.org/wmt16/} datasets).
\end{itemize}

In the following sections, we describe each in more detail.

\subsection{Khan Academy}

The data provided by Khan academy consist of English-Czech subtitles,
where the Czech part (usually manually translated from English) was manually
edited. During the analysis of the dataset,
we noticed that most of the time, the corrections were made
mostly on the lexical level which is to be expected since the Czech sentences
were created by a human translator.
Therefore, we concluded that this dataset has little to no value
for the task of training a model for correcting errors in morphology created by MT systems.

%Therefore, we decided to avoid using this dataset for the time being or to
%rather treat the corpus as a simple bilingual data.

\subsection{Autodesk}

Autodesk data consist of English sentences which were machine translated into
a set of target languages (cs, de, pl etc.) complemented with human post-editing
of the MT output. However, these datasets are domain specific (mostly user documentation),
so they might not be very attractive to use with more general texts.
We were not able to find any information about the MT system that was used
to create the translated output. The biggest advantage of these data is
their larger volume when compared with other post-editing datasets so we
used them mainly for model development and benchmarking of the used machine
learning methods.

\subsection{HimL-Lingea Logs}

The data provided by Lingea\footurl{http://www.lingea.cz/} were collected when official test sets for
HimL project were
created. The data consist mainly of texts related to public health.
The original English sentences were first
machine-translated to languages at which
the project was aimed (Czech, German, Polish and Romanian)
and then post-edited by professional
translators using Lingea's post-editing tool. The datasets are probably the most
detailed one since they consist of complete logfiles
describing elementary actions taken by human post-editors (such as selecting
phrases in a translated sentence, looking up alternative translations
in a dictonary etc.).

When we examined the data more closely,
we noticed that it is rather difficult to determine which actions are
useful for our machine learning process. Also, we were a little disappointed
when we found out that most of the time, the post-editors preferred to
rewrite the whole sentence \equo{from scratch}\footnote{By that, we mean that the
human post-editor usually prefered to rewrite the whole corrected sentence, even though
only a several changes (either lexical and morphological) were made, and delete the original.
This might have been also motivated by the need to reorder the MT output.}
instead of doing a few atomic modifications to the provided MT output.

In the end, we decided to simply extract triparallel data from these logs (the source
sentence + SMT output + result of the human post-editing). In the future,
we might consider to use other logged actions for model training.

\subsection{WMT Datasets}

For the last decade, the workshop on machine translation (WMT) has aimed
to provide working grounds for many researchers in the field of machine
translation. It has been great source of parallel data between English
on one side and various other languages on the other. Each year, the scope
of the workshop expands, including various new subtask related to machine translation,
such as several evaluation tasks and, more recently, automatic post-editing task.

The data available for the post-editing task usually contains a set of:
\begin{itemize}
    \item source English sentences,
    \item output of various MT systems, usually the ones that participate in the main
translation tasks,
    \item either a reference sentences or human post-editing of the MT output from the participating MT systems.
\end{itemize}

These datasets give us the opportunity to compare the performace of our post-editing
models when applied to the different systems. Even though the data available for the
WMT subtasks are often from various domains (news, IT, biomedical), the domain of the post-editing
data is more limited, mainly to the news articles.

\subsection{Other Sources}

The sources listed above (each one to a different degree) can
be considered a knowledge base for examining the behaviour of a human post-editors as well
as training data for our system. We think that they provided us with
some interesting insight into the post-editor's thought process. On the other hand,
we have also considered using other sources since the data mentioned above are quite
limited in size.

One possible way to tackle the shortage of training data is to use available parallel corpora.
These corpora (containing only \notion{source sentences + reference translations})
can be expanded by translating the source sentences and thus creating a set of 
sentences which contain MT generated errors and should be fixed to resemble the 
reference translation. These data can be then used to train post-editing models for that
specific SMT system\footnote{Obviously, the post-editing model training data have to be
different from the parallel data used for the training of the SMT system, so
some
jackknife sampling should be used with limited training data.} or possibly for
other SMT systems.
This method can surely help to overcome the aforementioned data acquisition bottleneck
since there is generally much more parallel data then post-edited sentences. For
English-Czech language pair, the natural choice of the parallel corpus would be
CzEng~1.0 \citep{czeng10:lrec2012}\footurl{http://ufal.mff.cuni.cz/czeng}.

%We did not use this approach in the scope of this thesis however, it might
%be considered in our future work.
%\todo{posledni vetu az do shrnuti?}

Of course, this aproach introduces some additional noise
related to the post-editing task. For example, we can get fluent and correct MT output which
is very different from the reference translation
because sentences can easily have hundreds of thousands of correct translations \citep{biblio8445677574576847760}.
%(possibly thanks to the different choice of wording etc.).
Therefore, it can be hard to distinguish which of the differences between
MT and the reference translation are genuine MT errors.
%these fairly correct training instances from the incorrect ones.

The basic summary of the available data is shown in the table \Tref{avail-data}.

\subsection{Monolingual Data}

We have also considered using bitext \emph{monolingual} data (either \notion{MT output + post-edited sentences}
or \notion{MT output + reference translations}), however due to the nature of our processing
pipeline we would be much more limited when analyzing the training data. Also this way, we would
lose the additional information that can be extracted from the source sentences, which proved to be valuable in the practice \citep{biblio:RoMaDEPFIXA2012}.


\begin{table*}[t]
\centering
\small

\begin{threeparttable}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\multirow{2}{*}{}  &  \multirow{2}{*}{\hash{} Sentences}  &  \multirow{2}{*}{\hash{} Tokens English}  &  \multicolumn{3}{c|}{\hash{} Tokens Czech}  \\
\cline{4-6}
&  &  & MT & PE & REF \\
\hline
Khan academy & $\sim$14k & $\sim$93k & $\sim$93k\tnote{*} & $\sim$93k & $-$ \\
\hline
Autodesk & 46,916 & 490,005 & 456,697 & 441,645 & $-$ \\
\hline
HimL-Lingea & 3892 & 60,142 & 51,428 & 56,485 & $-$ \\
\hline
WMT10 & 2,489 & 54,021 & 44,578 & $-$ & 45,422 \\
\hline
WMT16 & 2,999 & 57,418 & 48,037 & $-$ & 48,915 \\
\hline
CzEng 1.0 & 15M & 206M & $-$ & $-$ & 150M \\
\hline
\end{tabular}
\begin{tablenotes}
\item[*] Translations in Khan academy data come from humans.
\end{tablenotes}
\end{threeparttable}
\caption[Summary of the available data]{Summary of the available post-editing data. Only English-Czech data is listed, however, for datasets
where data for other language pairs are available, their volume is approximately the same. We provide only rough estimates for the Khan academy data.
There is no information about the number of tokens in the MT part of CzEng because we decided to abandon the
idea of creating a triparallel corpus for the time being.
}
\label{avail-data}
\end{table*}

%The
%number of parallel sentences and the number of tokens in the English source, the
%MT output (\equo{MT}) and the post-edited MT output (\equo{PE}). Only
%English-Czech data is listed since these datasets for other target languages
%(where available) are similar in volume.
%For Khan's school, we only
%provide estimates. The CzEng data set is not translated by any SMT at the moment,
%so the related information is omitted. We provide the information only for comparison.

%We do not include information about the QT21 data since we are yet to explore them.

