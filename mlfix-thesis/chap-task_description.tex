\chapter{Task Definition}
\label{chap:task_descr}

In this chapter we present results of the closer inspection
of the available training data and describe our process of developing
the MLFix post-editing component.

%issues:
% - what to classify
% - how to correct
% - what are the training instances
% - 

When we closely inspected the morphological errors present in our data we decided
to approach the post-editing problem as a classification task. 
Basically, the main idea is to identify words with incorrect surface form,
assign new morphological categories to such words (e.g. via new morphological tags, Interset
categories) and generate a new surface form.
This approach seems reasonable, since it gives us fair amount of freedom in generalizing the morphological
post-editor, by customizing the scope of the trained classifier or classifiers (e.g. by specifying the
set of categories being predicted by the classifier or the set of instances, which can be modified by the predictor).
On the other hand, it raises several issues that have to be resolved, mainly:
\begin{itemize}
    \item What instances should we extract from our training data?
    \item Which of the extracted instances should we mark as incorrect, as opposed
        to instances that represent words with correct surface from?
    \item Is it necessary to extract error-free training instances?
    \item What features should we extract for each instance?
    \item How should we apply the trained classifier? Should we apply it
        on each word in a sentence or should we identify the erroneous words
        first?
    \item What morphological categories should be predicted by our classifier?
\end{itemize}

Another important question is how should we measure the performance of our models.
Obviously, we cannot base the quality of our system on the performance of the
classifier itself since even a really well performing classifier can have only a small or negative
impact on the edited sentences (e.g. if we choose to predict morphological categories
that have very small impact on the final surface form, or we incorrectly identify erroneous words
in our training data). Still, the standard metrics
used for classifier evaluation, such as accuracy, precision and recall, can be helpful
during some stages of development (e.g. the choice of machine learning method, hyperparameter tuning).

However, in the end, our main goal is not producing a well performing classifier, but creating
more fluent, grammatically correct sentences.
Naturally, for this purpose, human evaluation of the post-edited sentences is the best choice as far
as reliable judgement goes, but it is also very costly and we usually need a more efficient
method during system development. Therefore, we rely on the widely used BLEU scoring and
probably even more suitable translation error rate (TER) metric. Relying on these methods alone
has however a few drawbacks which we will address later in the in this chapter.

\section{Specifying the Task}

As mentioned above, our goal is assigning new surface forms to the morphologically incorrect words
in the MT translated text via newly predicted morphological tags. For morphologically rich languages,
this can be quite difficult due to large tagsets. Also, most of the time,
the incorrect surface form of the word is only a result of an incorrect value in a small
subset of morphological characteristics of the word (e.g. wrong case, number, gender).
These reasons are why we decided to use the Interset categories instead\footnote{It is
not necessary for Czech, since the Czech positional tagset allows us to modify only necessary
parts of the morphological tag. However, other languages do not provide similar tagset, therefore
Interset might be more feasible representation.} and allow for predicting only a few categories at a time.

Eventually, when we were deciding on how our post-editing system should work, we have settled on three possible scenarios:
\begin{enumerate}
    \item Have one completely general classifier that is applied on each word in the sentence (some
        words might be omited by hand-crafted rule, e.g. ignore classes that do not flect).
    \item Identify the erroneous words first by a separate classifier. Apply a second classifier
        on the marked words and predict new morphological categories.
    \item Identify the erroneous words same way as in the previous scenario however, this time also choose a classifier that
        should be applied in the second step. The choice can be simply deterministic (e.g. via general POS), or stochastic.
\end{enumerate}

The first scenario can be appealing because it is very easy to implement and requires only one model.
The problem is that requirements for such model might be way too big and difficult to satisfy. Another
problem might be an unbalanced training dataset since most of the training examples will represent
\equo{correct} (as far as fluency of the translated text is concerned)
instances, where predicting a new morphological categories is not desired. Therefore
producing a well performing model can become difficult in the end.

We thus find the second scenario much more plausible and it was our main focus during this research.
It requires a simple binary classifier for the first step and one multiclass (or possibly multitask\footnote{We
define a multitask classifier as a classifier combining two classification tasks together (e.g. predicting new number and new case).
The tasks are separate, however the classifier takes the possible relation of the two tasks into account.}) classifier.
When we train the binary classifier (which identifies the words that need to be corrected) we still have to face
the issue of an unbalanced training dataset, on the other hand, the classifier assigning new morphological categories
can be simply trained only on the incorrect instances.

The third scenario seemed reasonable because we usually want to modify different morphological categories for different POSes.
However due to the results of the data analysis (presented in more detail in Section~\ref{sec:feat_extract}), we found it unnecessary to implement for the time being.

\subsection{Oracle Classifier}

Now that we have outlined the post-editing subtasks and assigned hypothetical classifiers 
to solve each one of them, there are following two issues left to resolve: the choice of
training instances and the choice of classifier targets. Since we only focus on correcting morphological
errors generated by the MT system and we ignore lexical errors completely, we have to be careful
during the extraction of training instances for our classifier. We have found it very helpful to
use a \equo{fake} classifier for this task, that we simply call \pojem{Oracle}.

The basic idea behind the Oracle classifier is that it has access not only to the
source sentences and the MT output like our production classifier but also to the \equo{correct} answers
contained in the \pojem{reference translations}/\pojem{post-edited sentences}. The most important
task for the Oracle is to help us to observe whether the suggested definition of \pojem{training instances} and
moreover the definition \pojem{incorrect instances} (the instances, which require post-editing)
has a potential of improving the MT output (if we had a perfect classifier).

We decided to extract one \pojem{training instance} for each word in our data that meets all of the following criteria:
\begin{itemize}
    \item the \pojem{MT-node lemma} IS\_EQUAL to the \pojem{REF-node lemma},
    \item the \pojem{MT-parent\_node} IS\_DEFINED AND\newline{}
        the \pojem{MT-parent\_node} IS\_NOT\_ROOT,
    \item the \pojem{REF-parent\_node} IS\_DEFINED AND\newline{}
        the \pojem{REF-parent\_node} IS\_NOT\_ROOT,
    \item the \pojem{SRC-node} IS\_DEFINED,
    \item the \pojem{SRC-parent\_node} IS\_DEFINED AND\newline{}
        the \pojem{SRC-parent\_node} IS\_NOT\_ROOT,
\end{itemize}
We have settled for these criteria for the following reasons: we want to extract only instances that have true predictions
available (thus the check for the presence of the aligned REF-node), we want to ignore misleading instances related
to possible lexical errors or different lexical choice (equality of the SRC-node and REF-node lemmas)
and finally, we want to make sure that
enough relevant tree-based context information will be extracted. We have not modified the definition
of the training instance much during the research. On the other hand, we experimented with several definitions
of the \pojem{incorrect instance}.

We tried three different heuristics to identify \pojem{incorrect instances}, each with a different set of conditions:
\begin{enumerate}
    \item the \pojem{MT-node form} IS\_NOT\_EQUAL to the \pojem{REF-node form},
    \item the \pojem{MT-node form} IS\_NOT\_EQUAL to the \pojem{REF-node form} AND the \pojem{MT-parent form} IS\_EQUAL to the \pojem{REF-node form},
    \item the \pojem{MT-node form} IS\_NOT\_EQUAL to the \pojem{REF-node form} AND (the \pojem{MT-parent form} IS\_EQUAL to the \pojem{REF-node form}
        OR the \pojem{MT-parent form} is marked as INCORRECT)
\end{enumerate}
All definitions work in the context of the \pojem{training instance}. For later reference, we refer to
these heuristics as \pojem{WrongForm1}, \pojem{WrongForm2} and \pojem{WrongForm3} respectively.
In any case, if the conditions
are not met, the training instance is marked as correct and should be left unchanged by the classifier.
These definitions cover both instances that should be changed (\pojem{correct} vs. \pojem{incorrect} instance)
and how the incorrect instances should be modified (values from the REF-node). The surface forms
generated with the use of these training instances should be identical to those of the reference,
if all morphological categories, that are different from the reference, are properly set before generating the new form.
This fact has one major drawback related to the evaluation:
the quality of the Oracle (and therefore the \equo{best}
possible result) or a production classifier cannot be reliably measured by automatic n-gram based metrics, because the post-edited
sentences will always have same or better score than the MT output. Therefore, manual evaluation is required
to some extent.

The first method (comparing only the surface forms) marked about one-tenth of the training instances as incorrect. When used in combination
with the Oracle classifier, the post-edited sentences have shown moderate improvement in the automatic metrics.
However in a closer observation of the sentences, we noticed many incorrect modifications such as~\Eref{ex-oracle-noref}.
Clearly, even though the wordform of \samp{místo} has been changed to match the reference,
the governing verb \samp{mít} requires its dependent to be in the dative case. The change
has thus actually introduced a new grammatical error into the sentence and thus worsened
the fluency of the translated sentence. Additionally, by changing 
the case of the word \samp{místo}, the already correct agreement with the subordinate adjectives
\samp{poslední} and \samp{volná} got also broken, making the result even worse. On the other hand,
the wordform in the reference sentence is correct because the governing word of \samp{míst}
is a noun (\samp{pár}) instead of a verb resulting in a genitive case of the word.

\begin{myexample}
    \small
    \catcode`\-=12
    \begin{center}
    \begin{tabular}{|l|p{0.75\textwidth}|}
    \hline
    \textbf{Source:}  &  \textbf{We have the last few vacancies for New Year's Eve and Christmas.}  \\
    \hline
    SMT output:  &  \textit{Máme} \textit{poslední} \textit{volná} \textbf{místa} na Silvestra a Vánoce.  \\
    \hline
    Gloss:  &  We \textit{have} the $\mathit{last_{dative}}$ $\mathit{few_{dative}}$ $\mathbf{vacancies_{dative}}$ for New Year's Eve and Christmas.  \\
    \hline
    Oracle output:  &  \textit{Máme} \textit{poslední} \textit{volná} \textbf{míst} na Silvestra a Vánoce.  \\
    \hline
    Gloss:  &  We \textit{have} the $\mathit{last_{dative}}$ $\mathit{few_{dative}}$ $\mathbf{vacancies_{genitive}}$ for New Year's Eve and Christmas.  \\
    \hline
    Reference:  &  Na Silvestra i na Vánoce \textit{máme} \textit{posledních} \textit{pár} \textit{míst}.  \\
    \hline
    \end{tabular}
    \label{ex-oracle-noref}
    \end{center}
\end{myexample}

The previous example has shown that for identifying incorrect instances, some additional information
about the surrounding members of the sentence is required.
We have seen that by slightly altering governing nodes, e.g.
by only choosing a different lexical translation of the source node or by choosing a completely different expression,
the equality between the surface form of the MT word and its reference counterpart cannot
be enforced without harming the quality of the MT sentence.
Therefore, we tried to introduce additional constraints to correctly identify candidates for post-editing.
To mark and instance as incorrect, not only the ref-node form has to be different from its aligned
ref-node form, but we must make sure that their governing nodes have identical surface form.
We considered checking only for the lemmas of the governing nodes at first, but this constraint
was too soft and was not able eliminate some of the previous errors.
The main motivation behind this constraint is to identify at least some agreement and possibly valency errors
without too much of language-specific insight. We assume that if the surface form of the mt-node and ref-node
differ while their parent node is identical, there is a high chance that the mt-node's surface form is
incorrect while the reference node has the right correction\footnote{This, of course, depends on the quality of the dependency
tree produced by the parser. However, since we use different parsing methods for mt-side (projection of the src-tree) and
the reference (dependency parser) we expect that the number of false positives in our training data will be lowered.}.


This additional constraint helped us to remove a large number of false positives and produced training examples
such as~\Eref{ex-oracle-parentref}. In this example, we can see that the noun \samp{život} was correctly changed
to \samp{života} because of the valency frame of the verb \samp{sdílet}. However, we can see, that this constraint
might be too strict because the adjective \samp{akademického} was left unmodified even though by changing its
governing noun \samp{život}, the agreement present in the MT output and should have been preserved after the
post-editing was left unnoticed. The post-editing of the word \samp{akademického} was omitted,
 because the surface forms of the governing nodes in the MT sentence and the reference sentence
(\samp{život} vs. \samp{života}) did not match. We observed this kind of false negative quite often which
led us to introducing one additional constraint.

\begin{myexample}
    \small
    \catcode`\-=12
    \begin{center}
    \begin{tabular}{|l|p{0.75\textwidth}|}
    \hline
    \textbf{Source:}  &  \textbf{…where he acknowledged the "wonderful people" he shared his academic life with.}  \\
    \hline
    SMT output:  &  …kde potvrdil, že je "skvělí lidé" \textit{sdílel} jeho akademického \textbf{života}.  \\
    \hline
    Gloss:  &  …where he acknowledged, that is "wonderful people" he \textit{shared} his academic $\mathbf{life_{genitive}}$.  \\
    \hline
    Oracle output:  &  …kde potvrdil, že je "skvělí lidé" \textit{sdílel} jeho akademického \textbf{život}.  \\
    \hline
    Gloss:  &  …where he acknowledged, that is "wonderful people" he \textit{shared} his academic $\mathbf{life_{dative}}$.  \\
    \hline
    Reference:  &  …do poděkování "skvělým lidem", s nimiž \textit{sdílel} akademický \textit{život}.  \\
    \hline
    \end{tabular}
    \label{ex-oracle-parentref}
    \end{center}
\end{myexample}

To soften the previous constraint in favor of not breaking the agreement of the dependent nodes, we decided
that if the previous constraint is not satisfied (possibly due to a difference between the mt-parent surface form
and the ref-parent surface form), the node in question should still be marked as incorrect if and only if
the governing node was marked as incorrect and the node's surface form is different form that of the reference
node. This is basically similar to post-editing the MT output dependency tree recursively from its
root to its leaf nodes.

Applying this new constraint we were able to produce training examples similar to~\Eref{ex-oracle-parentmark}.
As we can see in the MT output, there is correct agreement between the words \samp{Potrefená} and \samp{husa},\footnote{
Even though \samp{Potrefená Husa} is a named entity, in Czech, these are still flected with regard to the rest of the sentence
structure.} however, they have broken agreement with the governing preposition \samp{naproti}. This is corrected in the Oracle output.
Even more interesting is the phrase \samp{narazit na moravské náměstí} and its counterpart, \samp{narazit na moravském náměstí},
both being grammatically correct while having different meaning. The first one means literally \samp{to come across moravské náměstí}
(\samp{náměstí} meaning \samp{square} in Czech), while the other one can be translated as \samp{to come across (someone/something) on moravské náměstí}.
In this case the phrase created by the Oracle classifier (even though it does not work on the phrase-level) is closer
to the original meaning. We might notice, the some named entities were not correctly identified (\samp{Moravské}, \samp{Husa}) but
correcting these is not a goal of the original task.

\begin{myexample}
    \small
    \catcode`\-=12
    \begin{center}
    \begin{tabular}{|l|p{0.75\textwidth}|}
    \hline
    \textbf{Source:}  &  \textbf{The only place we've managed to come across is on Moravské náměstí, opposite the Potrefená Husa.}  \\
    \hline
    SMT output:  &  Jediné místo, kde se nám podařilo \textit{narazit na} \textbf{moravské} \textit{náměstí} \textit{naproti} \textbf{Potrefená} \textbf{husa}.  \\
    \hline
    Gloss:  &  The only place, where we've managed to \textit{come across} $\mathbf{Moravsk\acute{e}_{dative}}$ $\mathit{n\acute{a}m\check{e}st\acute{i}_{dative}}$ \textit{opposite the} $\mathbf{Potrefen\acute{a}_{nominative}}$ $\mathbf{Husa_{nominative}}$. \\
    \hline
    Oracle output:  &  Jediné místo, kde se nám podařilo \textit{narazit na} \textbf{moravském} \textit{náměstí} \textit{naproti} \textbf{Potrefené} \textbf{huse}.  \\
    \hline
    Gloss:  &  The only place, where we've managed to \textit{come across} $\mathbf{Moravsk\acute{e}_{locative}}$ $\mathit{n\acute{a}m\check{e}st\acute{i}_{locative}}$ \textit{opposite the} $\mathbf{Potrefen\acute{a}_{genitive}}$ $\mathbf{Husa_{genitive}}$.  \\
    \hline
    Reference:  &  Jediné místo, na které jsem zatím natrefil, je na \textit{Moravském náměstí naproti Potrefené Huse}.  \\
    \hline
    \end{tabular}
    \label{ex-oracle-parentmark}
    \end{center}
\end{myexample}

It should be noted, that these constraints do not detect all possible morphological errors. For example,
since we check mostly only the relationship with the governing node, we effectively omit subject-verb
agreement errors. This can be possibly
fixed in the future by additional constraints, however, in the scope of this thesis, our main objective was
to find and evaluate a method for identifying and correcting morphological errors that is as general as possible.

We also must keep in mind that the final constraint, while producing reasonable training examples,
still managed to produce instances containing false positives. Even though it might be caused by
incorrect assumptions during the designing of our heuristics, we should point out that there
are instances such as~\Eref{ex-oracle-ambig}, where it is difficult to decide whether the post-editing
helped to improve or harmed the fluency of the MT output, possibly due to being only one of the
several steps that needs to be taken during the post-editing.
In this example, we can see that the adjective \samp{osvětleného} was changed to \samp{osvětleném} because the
governing node \samp{baru}, being a part of the apposition with the \samp{Julep Room}, should
be in the preposition-noun agreement with the preposition \samp{v}. However due to mistranslation of the
named entity (\samp{Julep Room}) and wrong indication of the apposition relationship (missing comma),
the reader might consider the correction made by the Oracle less fluent or unnatural. On the other hand,
the meaning of the original MT output is quite different from the meaning in the source sentence, therefore,
some post-editing is definitely required.

As we have seen, the task of extracting meaningful and correctly annotated training instances for our system
 gets increasingly difficult with growing difference between the MT output and the reference sentences.
If provided output only from a poor MT systems, the task might be almost impossible. For this reason,
we have tried another, somewhat limited, method for creating and extracting training instances.

\begin{myexample}
    \small
    \catcode`\-=12
    \begin{center}
    \begin{tabular}{|l|p{0.75\textwidth}|}
    \hline
    \textbf{Source:}  &  \textbf{…entertainment coordinator at The Julep Room, a dimly lit bar near Gautier, said…}  \\
    \hline
    SMT output:  &  …programový koordinátor \textit{v} Julep \textit{místnosti} \textbf{osvětleného} \textit{baru} u Gautiera, prý…  \\
    \hline
    Gloss:  &  …entertainment coordinator at The Julep \textit{room} of a $\mathbf{lit_{genitive}}$ $\mathit{bar_{genitive}}$ near Gautier, said…  \\
    \hline
    Oracle output:  &  …programový koordinátor \textit{v} Julep místnosti \textbf{osvětleném} \textit{baru} u Gautiera, prý…  \\
    \hline
    Gloss:  &  …entertainment coordinator $\mathit{at_{locative}}$ The Julep room a $\mathbf{lit_{locative}}$ $\mathit{bar_{locative}}$ near Gautier, said…  \\
    \hline
    Reference:  &  …koordinátor zábavy \textit{v} \textit{The Julep Room}, spoře \textit{osvětleném} \textit{baru} poblíž Gautier, řekl…  \\
    \hline
    \end{tabular}
    \label{ex-oracle-ambig}
    \end{center}
\end{myexample}
%\todo{ukazat i rozdilne dep. struktury u ex. 4.4?}

\subsection{Depfix Reference}

From the previous observations, we assume that the constraint-based method achieves
best results when combined with data, where reference sentences
(or human post-edited sentences) are as close to the MT output as possible.
We can also take another look at the issue: the less is the amount of unnecessary changes
, or in our case changes which cannot be identified by our system (e.g. lexical error corrections),
more precise should be the extraction of the training instances.
For this reason, we have decided to examine training data, where we created \equo{syntetic} post-editating
of the MT output. These synthetic sentences are neither produced by a human translator
or created by a human post-editing process. A suitable tool for this task might quite naturally be Depfix.

The main focus of the Depfix post-editing tool is correcting morphological errors (aside from some
frequent lexical errors, e.g. missing reflexive particles and too eagerly translated named entities) which usually 
results in post-edited sentences that are not that different from the MT output. Aside from that,
the morphological changes (even though they are a result of an applied set of rules) made
by Depfix are very similar to the post-editing changes we are trying to teach our statistical component.

Considering these observations, we have created a new datasets by applying Depfix on the available bilingual data and
using Depfix output in place of reference sentences. We have then extracted the training instances in a
similar way as with the genuine post-editing data. We have also run our Oracle classifier on the data with
the resulting sentences matching the Depfix output most of the time.\footnote{The only cases when the Oracle classifier
did not correspond to the modification made by Depfix were when Depfix made one of its lexical corrections.}

This method seems quite viable for the task of identifying incorrect instances and learning the
right correction method because it should reflect at least the thought process behind creating the corrections rules.
It should be also aplicable on various MT systems because, as \citet{depfix:2014} has
previously reported, Depfix was able to improve the quality of various systems to some degree.
The main downside to this method is currently being
limited only to the Czech language since the Depfix was
created with the aim to post-edit English-Czech machine translation.
Furthermore, it is questionable whether the system trained on these data can surpass the performance
of the original post-editing system. 
This approach might become more interesting if a viable method of adapting the trained models
to another language pairs is devised in the future.

\subsection{Oracle Evaluation}

In this section, we present a brief evaluation of the Oracle classifier to show
what is the possible upper limit, that can be reached by our statistical components. This is
important, because the resulting model cannot perform well with regard to our demands, if the extracted
instances that are provided during training are already incorrectly classified.

\begin{table*}[t]
\centering
\small

\begin{tabular}{lccc}
Reference  &  WrongForm1  &  WrongForm2  &  WrongForm3  \\
\hline
Original  &  0.159  &  0.021  &  0.026  \\
Depfix  & 0.009  &  0.003  &  0.003  \\
\end{tabular}
\caption[Comparison of the heuristic rules]{
Overview of the portions of instances from WMT10 dataset marked as incorrect using different heuristic rules.
We compare original dataset containing reference translations and dataset where reference translations
were replaced by Depfix output.
}
\label{marked-stats}
\end{table*}


We present a brief statistic about the percentage of the instances that were marked by each
heuristic presented in~\Tref{marked-stats}. The statistic was computed over several different
datasets presented in the previous chapter (Autodesk, HimL, WMT10, WMT16). We also provide
similar statistics for the data created via Depfix. We can see that the portion of incorrect instances,
i.e. the instances which will serve as our training data,
is not very high. This is due to many sentences being considered correct by our heuristics (possibly because
they contained either too many errors, or the errors were of a different nature than the ones we detect).

To get a brief idea, if our heuristic has a potential of providing valuable information for the classifier
training, we decided to manually annotate a portion of the data from the HimL testset post-edited by our
Oracle classifier. Because this is not a final evaluation of our system, the evaluation was made only
by the author of this thesis. The evaluation has been done on the pairs of MT output, Oracle post-edited
output, randomly shuffled to lessen the bias of the annotator. We also provided the source sentence
to help the annotator decide whenever both outputs seemed grammatically correct but had a different meaning.
We did not provide the annotator with reference sentences due to the nature of Oracle post-editor. For comparison, we also extracted
a similar number of sentences from the same dataset, where the reference sentences were replaced
by the Depfix output. This evaluation was done mainly for the purpose
of development so only one annotator was involved.

\begin{table*}[t]
\centering
\small

\begin{tabular}{l|cc|ccc|cc}
Reference  &  Evaluated  &  Changed  &  $+$  &  $-$  &  0  &  Precision  &  Impact  \\
\hline
Post-edits  &  800  &  95  &  61  &  16  &  18  &  79.2\%  &  7.6\%  \\
Depfix  & 800  &  75  &  47  &  3  &  25  &  94.0\%  &  5.5\%  \\
\end{tabular}
\caption[Manual evaluation of the Czech Oracle classifier]{
Results of the manual evaluation of the ideal system based on the WrongForm3 heuristic.
Sentences were taken from HimL dataset. We compare the results on both original dataset
containing human post-edited sentences and
the dataset with Depfix reference sentences.
}
\label{oracle-maneval}
\end{table*}

In~\Tref{oracle-maneval}, we present results of the manual evaluation.
We present number of sentences in the evaluated dataset, the number of sentences,
that were actually modified by Oracle and the manually assigned label given to each modified sentence:
better ($+$), worse ($-$) or indecisive (0).
Indecisive annotations were usually a result of both translations being
too incomprehensive or due to multiple corrections in the sentence, both improving and worsening.
In addition, we have computed the precision~(\ref{eq:prec-or}) and
impact~(\ref{eq:rec-or}) of Oracle in a same manner as \citet{bojar-rosa-tamchyna:2013:WMT} did during Depfix evaluation in the Chimera MT system:

\begin{equation} \label{eq:prec-or}
precision = \frac{better}{better + worse}
\end{equation}
\begin{equation} \label{eq:rec-or}
impact = \frac{better}{evaluated}
\end{equation}

We can see from the results, that the final heuristic presented in this chapter,
if we ignore the indecisive corrections,
made mostly positive changes to the MT output (about four-fifths of the time)
However, the impact is quite low, only slightly more than 7\% of the sentences were modified
by Oracle classifier. When compared to the results achieved on the data with Depfix
reference sentences we can see that the precision grows even more
and the overall impact drops a little lower.
Therefore, we can assume that the presented heuristic can provide us with
quite reliable training data. Sadly, due to the low impact, much larger
volumes of post-edited data are needed to extract of reasonably big training dataset.

\section{Feature Extraction}
\label{sec:feat_extract}

%TODO: tabulky
% automatic statistic of extracted/filtered features (variance filter) - mozna jen kratke shrnuti
% popsat ruzne mnoziny featur + velikosti (extracted filtered) - src vs nosrc - az v dalsi kap ???
% add aggreement features ???

We have chosen the following strategy for designing the initial set of features:
we extract as many distinct features based on the available node information
and later we reduce the features by one or several methods
of features selection, either manual or stochastic.

Our feature set has a hierarchical structure. For each training instance, we extract
information about the node, its parent, the aligned source node and aligned source node's
parent. For training purposes, we also extract information from the aligned reference
node. Note, that we ignore information about the parent of the reference node, we
use this information only for distinguishing the incorrect instances from the correct ones
according to the previously described approach.

From each of these \samp{main} nodes inside the instance, we extract information
specific to them and to their close neighborhood (e.g. their parent, grandparent,
preceding child, following child, preceding sibling, following sibling). We
also extract information about the number of preceding and following children,
direction of the edge coming from the node's parent and finally the Interset representation
of the morphological POS tag. As a default, if a value is not defined we use an empty string instead.
We are allowed to do that because we treat every feature as categorical, i.e. taking one of discrete values.

We have also tried extracting the lemmas but it resulted in a
large growth of an already quite big feature space, so we initially abandoned the idea.
In the future, it might be interesting to include at least a limited number of the most frequent lemmas in our
feature set and observe how they can affect the performance of the classifiers.

This way, we extracted around 1500 initial features. Of course, this was a general feature
set that needed to be further processed within every classification task. Also, the method
by which we chose the initial features resulted in many of the features having zero
variance. Those features we removed, giving us around 1000 features.

% TODO: statistiky

