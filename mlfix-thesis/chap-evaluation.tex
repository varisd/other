\chapter{System evaluation}
\label{chap:eval}

In this chapter, we will present the results of the MLFix system evaluation.
We will describe the datasets we have used during the evaluation and the different
configurations of MLFix we have compared. Furthemore we will present the evaluation
of the individual MLFix components.
We will also present a comparison with the Depfix
system. We have performed both automatic and manual evaluation.

\section{Automatic evaluation}

During automatic evaluation, we have used mainly BLEU~\ref{papineni:2002} translation quality metric. However, we have also
measured Translation Edit Rate\ref{Snover06astudy} (TER) since it measures the amount of editing that is required by a human
to change the system output to match the reference translation. Even though the BLEU is widely used standard metric
used for SMT evaluation, we think that TER might be quite suitable for the evaluation too mainly because it indicates
how much aditional work is required to correct the SMT output.

We have evaluted MLFix on the following datasets: Autodesk, WMT10, WMT16, and HimL. All these datasets
were translated with the Chimera system. We have also measured the MLFix performance on the output of neural network
machine translation (NMT) system produced on the WMT16 dataset.

\todo{nejdriv komponenty, pak cely system s best configem?}
Since the domains of the datasets differ to a various degree, we have decided to combine the final models trained
on the different datasets. This combined model
compares result of each separate classifier (with the prediction probability) and picks the most probable choice.
We have used the combined model for the prediction of the new categories, for the error detection we have compared
the combined model with the best model


\subsection{Error detection evaluation}

\subsection{Category prediction evaluation}


\section{Manual evaluation}

For manual evaluation, we have used the HimL dataset. We have used two independent annotators, both were presented with 
with a set of sentences randomly selected from the dataset.

A total of \todo{cislo} sentences were evaluated by the annotators. The results are shown in\todo{table}.
%okec

%okec k agreementu
