\chapter{System Evaluation}
\label{chap:eval}

In this chapter, we present the results of the final MLFix system evaluation.
We describe additional datasets we used during the evaluation and different
configurations of MLFix we compared. Furthemore, we present evaluation
of the individual MLFix components.
We also present a comparison with the Depfix
system. We performed both automatic and manual evaluation.

\section{Automatic Evaluation}

During automatic evaluation, we used BLEU \citep{papineni:2002} translation
quality metric based on measuring the n-gram difference between the MT output
and the reference translation.
Even though it has its limitations, it is the most widely used
evaluation metric at the moment and it is considered a standard metric for automatic evaluation.
We relied mainly on the BLEU scoring metric, however, we also need to mention
Translation Edit Rate (TER) \citep{Snover06astudy} which we used during system development because it measures
the quality of translation based on the amount of corrections needed to match
the reference translation. Lowering the amount of work needed to post-edit the MT output
is one of the aims of MLFix system. Furthermore, the metric has been proved to provide
reasonable correlation with a human judgement.

We evaluted MLFix on the following datasets: Autodesk, WMT10\linebreak
\citep{callisonburch-EtAl:2010:WMT}, WMT16 \citep{bojar-EtAl:2016:WMT1}, and HimL.
These datasets
were translated with various SMT systems.

Because the domains of the training datasets differ to a various degree,
we decided to combine the final models instead of evaluating them separately.
The methods for combining the output of multiple classifiers
differ with each MLFix component and are described in more detail in the following
sections.
Naturally, for each dataset we exclude the models trained on the corresponding dataset from
the evaluation.

%This combined model
%compares result of each separate classifier (with the prediction probability) and picks the most probable choice.
%We have used the combined model for the prediction of the new categories, for the error detection we have compared
%the combined model with the best model

\subsection{Morphology Prediction Evaluation}

We performed the evaluation of the morphological prediction module first to determine
which combined model to use during the error detection evaluation. We compared
several model combinations: \pojem{Case} models only, \pojem{CN} models only, \pojem{CNG} models only, 
\pojem{CNGA} models only and a combination of all available models (\pojem{Comb}).
%We also
%made evaluation of combined models, which did not use the Depfix data (\pojem{Comb-D}).
Each combined model runs all classifiers separately and stores the probabilities of their
predictions. Using these predictions, the candidate tags are generated and are scored by 
these probabilities. If a tag is predicted by multiple classifiers, the scores are summed together.
The tag with the highest score is then chosen a result of the combined model.

We present the final results of the morphological prediction module evaluation in~\Tref{fixonly-summary}.
We compare BLEU scores of each model combination and their improvement over the baseline scores.
The values are multiplied by 100 for easier reading. We can see that the CNG
and CNGA models had a largest impact most of the time. Since the difference between the CNG and CNGA
performance is very small we chose CNG model for the final evaluation because its models
performed better during the model training.

Surprisingly, the performance of the Comb models was lower than we anticipated. This might be caused
by the fact that these models use basically four times more classifiers and no weighting is used
when combining their results.
Therefore, the final results of the combined models may be biased towards the simpler models working
with lesser amount of target values thus assigning their predictions a higher score. Still, a more thorough
investigation is needed in the future.

\begin{table*}[t]
\centering
\small
\resizebox{0.98\textwidth}{!}{

\begin{tabular}{|l|l||c||c||c|c|c|c|c|}
\hline
Dataset  &  System  &  Oracle  &  Base  &  Case  &  CN  & CNG  &  CNGA  & Comb  \\
\hline
\hline
Autodesk  &  NA  &  49.20  &  47.82  &  47.89 (+0.07)  &  47.91 (+0.09)  &  \bf{48.22} (+0.40)  &  48.21 (+0.39)  &  47.90 (+0.08)  \\
\hline
HimL  &  Moses &  23.33  &  20.66  &  21.14 (+0.48)  &  20.97 (+0.31)  &  21.36 (+0.70)  &  \bf{21.49} (+0.83)  &  21.03 (+0.37)  \\
\hline
WMT10  &  CU Bojar  &  16.70  &  15.66  &  15.84 (+0.18)  &  15.82 (+0.16)  &  \bf{15.95} (+0.29)  &  \bf{15.95} (+0.29)  &  15.82 (+0.16)  \\
\hline
\multirow{2}{*}{WMT16}  &  UEDIN NMT  &  27.31  &  26.31  &  26.43 (+0.12)  &  26.48 (+0.17)  &  26.47 (+0.16)  &  \bf{26.50} (+0.19)  &  26.44 (+0.13)  \\
&  CU Chimera  &  23.13  &  21.72  &  21.94 (+0.22)  &  21.99 (+0.27)  &  22.02 (+0.30)  &  \bf{22.05} (+0.33)  &  22.02 (+0.30)  \\
\hline
\hline
Autodek-D  &  NA  &  98.32  &  96.93  &  98.12 (+1.19)  &  98.09 (+1.16)  &  \bf{98.17} (+1.24)  &  98.16 (+1.23)  &  98.08 (+1.15)  \\
\hline
HimL-D  &  Moses  &  96.05  &  94.23  &  95.23 (+1.00)  &  95.04 (+0.81)  &  95.41 (+1.18)  &  \bf{95.42} (+1.19)  &  94.98 (+0.75)  \\
\hline
WMT10-D  &  CU Bojar  &  95.72  &  93.42  &  95.03 (+1.61)  &  94.99 (+1.57)  &  \bf{95.10} (+1.68)  &  95.07 (+1.65)  &  94.96 (+1.54)  \\
\hline
WMT16-D  &  CU Chimera  &  97.51  &  96.56  &  97.22 (+0.66)  &  97.23 (+0.67)  &  97.26 (+0.70)  &  \bf{97.27} (+0.71)  &  97.20 (+0.64)  \\
\hline
\end{tabular}

}
\caption[Automatic evaluation of the Czech morphological prediction module]{
    Automatic evaluation of the morphological prediction module using BLEU metric
	and the relative improvement over the baseline MT output.
	Values are multiplied by 100 for easier reading.
	Performance of 
	the Oracle classifier is also provided for comparison. Datasets with the \pojem{-D} suffix have Depfix output in place of reference sentences.
	The best model for each dataset is printed in bold.
}
\label{fixonly-summary}
\end{table*}


\subsection{Error Detection Evaluation}

We performed the error detection evaluation in a similar way. We used a combination
of binary classifiers in our prediction module. However, we chose a different strategy
for combining the output of these classifiers.
We decided to use voting scheme to determine the final prediction.
In the end, we compared three basic methods: \pojem{Majority} vote, \pojem{AtLeastOne} method,
and \pojem{Average} prediction method.\footnote{Surely, there are other voting strategies available
but they were not investigated in the scope of this thesis.}

The \pojem{Majority} vote basically chooses the class that was marked by a majority of classifiers.
We think that this way can help us compensate for the low precision of some of the classifiers we trained.
Because the classifiers were trained on different datasets, there should be some level of complementarity
between them. Still, we did not perform more thorough analysis to support this assumption.
A potential drawback of this method can be combined low recall of the error detection module. We assume that
combining several low recall classifers can further bias the module toward the majority class when using
this method.

The \pojem{AtLeastOne} method classifies the word as incorrect if at least one classifier marked it
as incorrect. This method should counter the problem of combining several low recall classifiers to some
extend, however, we expect the precision to drop rapidly when adding more classifiers, especially if their own
precision is low to begin with.

Because we are using classification models that support weighting of the predicted classes, we
also considered the \pojem{Average} voting scheme. If the combined classifier provide us not only with
a predicted class but also a some sort of confidence value (e.g. probability of the prediction correctness),
we can average these values and mark instance as incorrect if the averaged value exceeds a certain threshold.
This method is similar to the \pojem{Majority} scheme with a difference that it can choose a result
predicted by a minority when their overall confidence of the prediction is large enough.

We present the comparison of these three methods in~\Tref{markonly-summary}. Morphological values of the \equo{incorrect}
words were predicted by the Oracle morphological predictor, but only a morphological case, number and gender were predicted.
We can notice that the AtLeastOne method achieved
significantly better BLEU score than the other two. This was expected and in this case it does
not necessarily mean that the resulting sentences improved from the human evaluation standpoint, because
the situation is similar to the heuristic selection we presented in the task definition. Even though
many wordforms were changed by this method to reflect the wordforms in the reference sentence, it
might have lowered the overall fluency of the translated text. On the other hand, the BLEU scores were
still under the original WrongForm3 Oracle classifier threshold, therefore, the method seems promising.

As for the other two methods, the Average
voting method seems to perform slightly better than the Majority voting but the difference is not significant.

\begin{table*}[t]
\centering
\small
\resizebox{0.98\textwidth}{!}{

\begin{tabular}{|l|l||c||c||c|c|c|}
\hline
Dataset  &  System  &  Oracle  &  Base  &  Majority  &  AtLeastOne  &  Average  \\
\hline
\hline
Autodesk  &  NA  &  49.20  &  47.82  &  47.89 (+0.06)   &  \bf{48.47} (+0.65) &  47.89 (+0.06)  \\
\hline
HimL  &  Moses  &  23.33  &  20.66  &  20.69 (+0.02)  &  \bf{22.08} (+1.41)  &  20.69 (+0.02)  \\
\hline
WMT10  &  CU Bojar  &  16.70  &  15.66  &  15.84 (+0.18)  &  \bf{16.30} (+0.64)  &  15.77 (+011)  \\
\hline
\multirow{2}{*}{WMT16}  &  UEDIN NMT  &  27.31  &  26.31  &  26.31 (0)  &  \bf{26.49} (+0.18)  &  26.31 (0)  \\
&  CU Chimera  &  23.13  &  21.72  &  21.79 (+0.07)  &  \bf{22.01} (+0.28)  &  21.79 (+0.07)  \\
\hline
\hline
Autodek-D  &  NA  &  98.32  &  96.93  &  97.99 (+1.05)  &  \bf{97.99} (+1.05)  &  98.31 (+1.37)  \\
\hline
HimL-D  &  Moses  &  96.05  &  94.23  &  94.62 (+0.39)  &  \bf{96.04} (+1.81)  &  94.62 (+0.39)  \\
\hline
WMT10-D  &  CU Bojar  &  95.72  &  93.42  &  94.76 (+1.33)  &  \bf{95.61} (+2.18)  &  94.80 (+1.37)  \\
\hline
WMT16-D  &  CU Chimera  &  97.51  &  96.56  &  97.04 (+0.48)  &  \bf{97.47} (+0.9)  &  97.05 (+0.49)  \\
\hline
\end{tabular}

}
\caption[Automatic evaluation of the Czech error detection module]{
    Automatic evaluation of the error detection module using different voting methods
	to interpret output of multiple models using BLEU metric.
	Values are mutliplied by 100 for easier reading.
	For comparison, performance of Oracle classifier is also provided.
	Values in brackets indicate the difference between the method and the baseline (Base) MT output.
	Datasets with the \pojem{-D} suffix have Depfix output in place of reference sentences.
}
\label{markonly-summary}
\end{table*}

\section{System-wide Evaluation}

In this section, we present the final evaluation of MLFix system. Based on the evaluation of the
separate components, we chose CNG model combination for morphological prediction and we
decided to compare both AtLeastOne and Average voting method for error detection. We
also made comparison with the Depfix system. The results are summarized in~\Tref{final-summary}.
The amount of changed sentences by each system is shown in~\Tref{final-chgd}.

The results confirm that AtLeastOne voting method, while having large impact, does not work
quite well with Czech morphological prediction module. We cannot tell how many of the marked instances were actually
correct, nevertheless, the morphology correction had probably hard time predicting correct morphological categories
for these instances. On the other hand, we can see that MLFix with the Average voting method
was able to improve every tested MT output. However, the improvement was
usually quite small, always smaller when compared to Depfix. Interestingly,
Depfix was not quite able to improve output of the neural network machine translation system (UEDIN NMT),
while MLFix achieved positive score improvement. It might be only a coincidence and it needs to be investigated more thoroughly
in the future. Depfix results on the CU-Chimera dataset should be taken with a grain of salt since
Depfix is already part of the Chimera SMT system. Depfix also had a problem with Autodesk dataset
but we think this might be caused by the specific domain of the dataset (user documentation).

Even though the changes were positive according to the automatic evaluation, the impact of MLFix was
very low. If we compare the number of sentences changed by MLFix and Depfix, we can see that Avg-CNG MLFix
modified five to ten times less sentences depending on the dataset. We think that this might be
mainly due to the poor performance of the error detection classifiers in our error detection module.
Furthermore, Depfix covers wider range of corrections which also relfects in a larger overall impact.

\begin{table*}[t]
\centering
\small
\resizebox{0.98\textwidth}{!}{

\begin{tabular}{|l|l||c||c|c|c|c|}
\hline
Dataset  &  System  &  Base  &  ALO-CNG  &  Avg-CNG  &  Depfix  \\
\hline
\hline
Autodesk  &  NA  &  47.82  &  44.94 (-2.87)  &  \bf{47.89} (+0.06)  &  47.63 (-0.19)  \\
\hline
HimL  &  Moses  &  20.66  &  18.91 (-1.75)  &  20.69 (+0.02)  &  \bf{21.02} (+0.35)\\
\hline
WMT10  &  CU Bojar  &  15.66  &  14.61 (-1.04)  &  15.76 (+0.10)  &  \bf{15.91} (+0.25)  \\
\hline
\multirow{2}{*}{WMT16}  &  UEDIN NMT  &  26.31  &  25.75 (-0.55)  &  \bf{26.49} (+0.18)  &  26.15 (-0.15)  \\
&  CU Chimera  &  21.72  &  21.44 (0.27)  &  \bf{21.79} (+0.07)  &  21.75 (+0.02)  \\
\hline
\end{tabular}

}
\caption[Final Czech MLFix evaluation]{
	Final evaluation of MLFix using BLEU metric. Values are multiplied by 100 for easier reading.
	AtLeastOne (ALO-CNG) and Average (Avg-CNG) voting methods
	were compared. Performance of Depfix is shown for comparison.
}
\label{final-summary}
\end{table*}

\begin{table*}[t]
\centering
\small

\begin{tabular}{|l|l||c|c|c||c|}
\hline
Dataset  &  System  &  ALO-CNG  &  Avg-CNG  &  Depfix  &  Sent.  \\
\hline
\hline
Autodesk  &  NA  &  20,104  &  7,203  &  11,415  &  42,259  \\
\hline
HimL  &  Moses  &  473  &  30  &  261  &  800  \\
\hline
WMT10  &  CU Bojar  &  1,441  &  202  &  973  &  2,489  \\
\hline
\multirow{2}{*}{WMT16}  &  UEDIN NMT  &  668  &  86  &  418  &  2,999  \\
&  CU Chimera  &  713  &  122  &  624  &  2,999  \\
\hline
\end{tabular}

\caption[Final Czech MLFix evaluation - number of changed sentences]{
	Number of sentences changed by different systems. Total number of
sentences in each dataset (\pojem{Sent.}) is listed for reference.
}
\label{final-chgd}
\end{table*}



\section{Manual Evaluation}

For manual evaluation, we used the HimL dataset translated by Moses and the WMT16 CU Chimera dataset.
They were post-edited by the Avg-CNG MLFix configuration.
We used two independent annotators, both were presented with pairs
of MT output and MLFix output which were randomly shuffled. The source sentence and reference translation were also provided for each pair.
In the end, total of 126 changed sentences was evaluated. The results are shown in~\Tref{maneval-final}.

The results show that MLFix was able to improve around four-fifths of the modified sentences. This
result is quite pleasant, but it still needs to be confirmed in the future by a larger scale manual
evaluation. As we expected, the overall impact of MLFix is quite low, most likely due to the beforementioned
low recall of the error detection classifiers in the error detection module.
This module therefore requires further improvement in the future.

Both annotators evaluated the same set of 126 sentences, their inter-annotator agreement is shown~\Tref{maneval-agree}.
The inter-annotator agreement reached 62\% (87\%, if we disregard the indefinite changes),
therefore, the results of manual evaluation cannot be completely relied on this time.

\begin{table*}[t]
\centering
\small

\begin{tabular}{l|cc|ccc|cc}
  &  Evaluated  &  Changed  &  $+$  &  $-$  &  0  &  Precision  &  Impact  \\
\hline
HimL-A  &  800  &  26  &  17  &  5  &  4  &  77.2\%  &  2.1\%  \\
WMT16-A  &  2,999  &  100  &  73  &  21  &  6  &  77.6\%  &  2.4\%  \\
HimL-B  &  800  &  26  &  5  &  5  &  16  &  50\%  &  0.6\%  \\
WMT16-B  &  2,999  &  100  &  71  &  12  &  17  &  85.5\%  &  2.3\%  \\
\hline
Total &  7,598  &  252  &  166  &  43  &  43  &  79.4\%  &  2.1\%  \\
\end{tabular}
\caption[Czech MLFix manual evaluation]{
Results of the manual evaluation of best MLFix configuration (Avg-CNG). Annotators
A and B are distinguished by a suffix for each dataset.
}
\label{maneval-final}
\end{table*}


\begin{table*}[t]
\centering
\small

\begin{tabular}{c|cc|c}
 A/B  &  $+$  &  $-$  &  0  \\
\hline
$+$  &  64  &  7  &  5  \\
$-$  &  5  &  13  &  0  \\
\hline
0 &  21  &  5  &  6  \\
\end{tabular}
\caption[Czech MLFix manual evaluation - inter-annotator agreement]{
	Matrix containing inter-annotator agreement of MLFix manual evaluation.
}
\label{maneval-agree}
\end{table*}

